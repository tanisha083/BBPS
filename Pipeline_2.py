# -*- coding: utf-8 -*-
"""MAIN.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1O_mQNUmbrotwtDazKmlOn2bMlTo3C4Tg


"""


# import and Preprocess
import os
import numpy as np
import tensorflow as tf
import keras
from tensorflow.keras.preprocessing.image import load_img, img_to_array

import zipfile
from google.colab import drive

drive.mount('/content/drive/')

zip_ref = zipfile.ZipFile("/content/drive/My Drive/Boston Scoring/Dataset_final.zip", 'r')

zip_ref.extractall("/tmp")
zip_ref.close()

os.path.isdir('/tmp')

train_dir = '/tmp/Dataset/train'
val_dir = '/tmp/Dataset/val'
test_dir = '/tmp/Dataset/test'

image_size = (224, 224)
batch_size = 4
frames_per_video = 100  # Number of frames to sample from each video




#Custom Data Generator for Frame-level Deep Learning Model Training




class FrameDataGenerator(keras.utils.Sequence):

    def __init__(self, directory, image_size, frames_per_video, batch_size=8, shuffle=True):
        'Initialization'
        self.directory = directory
        self.image_size = image_size
        self.frames_per_video = frames_per_video
        self.batch_size = batch_size
        self.shuffle = shuffle
        self.classes = sorted(os.listdir(self.directory))
        self.num_classes = len(self.classes)
        self.video_folders = [os.path.join(directory, class_name, video_folder)
                              for class_name in self.classes
                              for video_folder in sorted(os.listdir(os.path.join(directory, class_name)))]
        self.on_epoch_end()

    def load_and_preprocess_image(self, frame_path):
        img = load_img(frame_path, target_size=self.image_size)
        img_array = img_to_array(img) / 255.0  # Normalize pixel values
        return img_array

    def __len__(self):
        'Denotes the number of batches per epoch'
        return int(np.floor(len(self.video_folders) / self.batch_size))

    def __getitem__(self, index):
        'Generate one batch of data'
        video_folders_temp = self.video_folders[index*self.batch_size:(index+1)*self.batch_size]

        X, y = [], []

        for video_folder in video_folders_temp:
            video_frames = sorted(os.listdir(video_folder))
            step = len(video_frames) // self.frames_per_video
            if step:
                sample_indices = range(0, len(video_frames), step)[:self.frames_per_video]
                frames = np.array([self.load_and_preprocess_image(os.path.join(video_folder, video_frames[index])) for index in sample_indices])
                X.append(frames)
                class_index = self.classes.index(os.path.basename(os.path.dirname(video_folder)))
                labels = np.squeeze(tf.keras.utils.to_categorical(class_index, self.num_classes))
                y.append(labels)

        return np.array(X), np.array(y)

    def on_epoch_end(self):
        'Updates indexes after each epoch'
        if self.shuffle == True:
            np.random.shuffle(self.video_folders)

!find '/tmp/Dataset/train/' -name "*.DS_Store" -type f -delete

train_gen = FrameDataGenerator(train_dir,image_size,frames_per_video,batch_size,shuffle=True)

!find '/tmp/Dataset/val/' -name "*.DS_Store" -type f -delete

val_gen = FrameDataGenerator(val_dir,image_size,frames_per_video,batch_size,shuffle=True)

!find '/tmp/Dataset/test/' -name "*.DS_Store" -type f -delete

test_gen = FrameDataGenerator(test_dir,image_size,frames_per_video,1,shuffle=True)

len(test_gen)


!pip install ipywidgets

import cv2
import os
import ipywidgets as widgets
from IPython.display import display

def play_video(frame_paths):
    # Get the first frame to obtain video properties
    first_frame = cv2.imread(frame_paths[0])
    # Check if the first frame is valid
    if first_frame is None:
        print("Failed to read the first frame.")
        return
    height, width, _ = first_frame.shape

    # Create a VideoWriter object to save the frames as a video
    output_path = 'output_video.mp4'  # Specify the output video path
    fourcc = cv2.VideoWriter_fourcc(*'mp4v')  # Specify the codec (e.g., 'mp4v' for mp4)
    fps = 30  # Specify the frames per second
    out = cv2.VideoWriter(output_path, fourcc, fps, (width, height))

    # Iterate over the frame paths and write each frame to the video
    for frame_path in frame_paths:
        frame = cv2.imread(frame_path)
        out.write(frame)

    # Release the VideoWriter
    out.release()

    # Create a video player widget
    video_player = widgets.Video.from_file(output_path, autoplay=True, loop=True)

    # Display the video player widget
    display(video_player)

!pip install remotezip tqdm opencv-python opencv-python-headless tf-models-official



#Setting up MoViNet Model for Transfer Learning




import tqdm
import random
import pathlib
import itertools
import collections

import cv2
import numpy as np
import remotezip as rz
import zipfile
import seaborn as sns
import matplotlib.pyplot as plt

import keras
import tensorflow as tf
import tensorflow_hub as hub
from tensorflow.keras import layers
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.losses import SparseCategoricalCrossentropy

# Import the MoViNet model from TensorFlow Models (tf-models-official) for the MoViNet model
from official.projects.movinet.modeling import movinet
from official.projects.movinet.modeling import movinet_model




"""##model"""

gru = layers.GRU(units=4, return_sequences=True, return_state=True)

inputs = tf.random.normal(shape=[4, 100, 3]) # (batch, sequence, channels)

result, state = gru(inputs) # Run it all at once

first_half, state = gru(inputs[:, :5, :])   # run the first half, and capture the state
second_half, _ = gru(inputs[:,5:, :], initial_state=state)  # Use the state to continue where you left off.

print(np.allclose(result[:, :5,:], first_half))
print(np.allclose(result[:, 5:,:], second_half))

model_id = 'a0'
resolution = 224
batch_size = 4
num_frames = 100  # length of your video sequence
num_classes = 4

tf.keras.backend.clear_session()

backbone = movinet.Movinet(model_id=model_id)
backbone.trainable = True


# Set num_classes=600 to load the pre-trained weights from the original model
model = movinet_model.MovinetClassifier(backbone=backbone, num_classes=600)
model.build([None, None, None, None, 3])

# Load pre-trained weights
!wget https://storage.googleapis.com/tf_model_garden/vision/movinet/movinet_a0_base.tar.gz -O movinet_a0_base.tar.gz -q
!tar -xvf movinet_a0_base.tar.gz






#Building Classifier and Learning Rate Scheduler for MoViNet Model Training





checkpoint_dir = f'movinet_{model_id}_base'
checkpoint_path = tf.train.latest_checkpoint(checkpoint_dir)
checkpoint = tf.train.Checkpoint(model=model)
status = checkpoint.restore(checkpoint_path)
status.assert_existing_objects_matched()

def build_classifier(batch_size, num_frames, resolution, backbone, num_classes=4):
  """Builds a classifier on top of a backbone model."""
  model = movinet_model.MovinetClassifier(
      backbone=backbone,
      num_classes=num_classes)
  model.build([batch_size, num_frames, resolution, resolution, 3])

  return model

def lr_scheduler(epoch, lr):
    # log the current learning rate onto W&B
    if wandb.run is None:
        raise wandb.Error("You must call wandb.init() before WandbCallback()")

    wandb.log({'learning_rate': lr}, commit=False)

    if epoch < 7:
        return lr
    else:
        return lr * tf.math.exp(-configs['lr_decay_rate'])

lr_callback = tf.keras.callbacks.LearningRateScheduler(lr_scheduler)

!pip install wandb

!pip install tensorflow-addons



#Training the MoViNet model with Wandb Integration 



import tensorflow as tf
import tensorflow_addons as tfa



"""##train

"""

import wandb
run = wandb.init(project='Segment',
    config={
    "metrics" : ['acc',tf.keras.metrics.AUC(),tf.keras.metrics.Recall(),tf.keras.metrics.Precision(),tfa.metrics.F1Score(num_classes=4, average="macro")],
    "model_name" : 'Movinet',
    "loss_fn" : 'categorical_crossentropy',
    "epochs": 5,
    },sync_tensorboard=True)
config = wandb.config

sgd = tf.keras.optimizers.SGD(learning_rate=3e-4, momentum=0.8, nesterov=False)

import tensorflow_addons as tfa
model = build_classifier(batch_size, num_frames, resolution, backbone, num_classes=num_classes)

num_epochs = 5

loss_obj = tf.keras.losses.CategoricalCrossentropy(from_logits=True)

optimizer = tf.keras.optimizers.Adam(learning_rate = 0.001)

cp_callback

model.compile(loss=loss_obj, optimizer=optimizer, metrics = ['acc',tf.keras.metrics.AUC(),tf.keras.metrics.Recall(),tf.keras.metrics.Precision(),tfa.metrics.F1Score(num_classes=4, average="macro")])

results = model.fit(train_gen,
                    validation_data=val_gen,
                    epochs=num_epochs,
                    validation_freq=1,
                    verbose=1,callbacks=[cp_callback,WandbCallback(),TensorBoard(log_dir=wandb.run.dir)])

model.evaluate(test_gen, return_dict=True)

model.save_weights('movinet_20epochs.h5')

model.save('saved_model/my_model_6')




!cp -r "/content/saved_model/my_model_6" "/content/drive/MyDrive"

from tensorflow import keras

new_model = tf.keras.models.load_model('/content/drive/MyDrive/my_model_6')

new_model=tf.saved_model.load('/content/drive/MyDrive/my_model_6')

new_model.get_weights()

model.load_weights('/content/drive/MyDrive/Boston Scoring/Movinet-best.h5')



"""#evaluation and prediction

"""

model.summary()



#Obtaining Actual and Predicted Labels from a Dataset




def get_actual_predicted_labels(dataset):
  """
    Create a list of actual ground truth values and the predictions from the model.

    Args:
      dataset: An iterable data structure, such as a TensorFlow Dataset, with features and labels.

    Return:
      Ground truth and predicted values for a particular dataset.
  """
  actual = [np.argmax(labels, axis=-1) for _, labels in dataset]
  predicted = model.predict(dataset)
  predicted = tf.argmax(predicted, axis=-1)

  print(actual)
  print(predicted)

  return actual, predicted



# Plotting Confusion Matrix for Actual and Predicted Labels




def plot_confusion_matrix(actual, predicted, labels, ds_type):
  cm = tf.math.confusion_matrix(actual, predicted)
  print(cm)
  ax = sns.heatmap(cm, annot=True, fmt='g')
  sns.set(rc={'figure.figsize':(6,6)})
  sns.set(font_scale=1.4)
  ax.set_title('Confusion matrix for ' + ds_type +' set')
  ax.set_xlabel('Predicted Segment')
  ax.set_ylabel('Actual Segment')
  plt.xticks(rotation=90)
  plt.yticks(rotation=0)
  ax.xaxis.set_ticklabels(labels)
  ax.yaxis.set_ticklabels(labels)
  # plt.show()
  plt.savefig('cm.svg')

true_classes = []
for _, labels in test_gen:
    true_classes.extend(np.argmax(labels, axis=1))

class_labels = list(set(true_classes))

actual, predicted = get_actual_predicted_labels(test_gen)
plot_confusion_matrix(actual, predicted, class_labels, 'test')

!pip install shap==0.40.0




# Generating Grad-CAM Heatmap for a given image and model





import math
from typing import Dict, Optional

import matplotlib.pyplot as plt
import numpy as np
import shap

# !pip install shap
from skimage.segmentation import slic
from matplotlib.colors import LinearSegmentedColormap
from tensorflow.keras.models import Model

!pip3 install keras-visualizer

#PRINT ALL LAYER NAMES FOR CAM
for idx in range(len(model.layers)):
  print(model.get_layer(index = idx).name)

import numpy as np
import tensorflow as tf
from tensorflow import keras

# Display
from IPython.display import Image, display
import matplotlib.pyplot as plt
import matplotlib.cm as cm

model_builder = model
img_size = (224,224)

last_conv_layer_name = "movinet"
# The local path to our target image
img_path='/tmp/Dataset/train/transverse/case10_1/case_M_20181113100307_0U62372111373406_1_005_001-1_a14_ayy_image0006.jpg' #healthy

def get_img_array(img_path, size):
    img = keras.preprocessing.image.load_img(img_path, target_size=size)
    # `array` is a float32 Numpy array of shape (299, 299, 3)
    array = keras.preprocessing.image.img_to_array(img)
    # We add a dimension to transform our array into a "batch"
    array = np.expand_dims(array, axis=0)
    return array


def make_gradcam_heatmap(img_array, model, last_conv_layer_name, pred_index=None):
    # First, we create a model that maps the input image to the activations
    # of the last conv layer as well as the output predictions
    grad_model = tf.keras.models.Model(
        [model.inputs], [model.get_layer(last_conv_layer_name).output, model.output]
    )

    # Then, we compute the gradient of the top predicted class for our input image
    # with respect to the activations of the last conv layer
    with tf.GradientTape() as tape:
        last_conv_layer_output, preds = grad_model(img_array)
        if pred_index is None:
            pred_index = tf.argmax(preds[0])
        class_channel = preds[:, pred_index]

    # This is the gradient of the output neuron (top predicted or chosen)
    # with regard to the output feature map of the last conv layer
    grads = tape.gradient(class_channel, last_conv_layer_output)

    # This is a vector where each entry is the mean intensity of the gradient
    # over a specific feature map channel
    pooled_grads = tf.reduce_mean(grads, axis=(0, 1, 2))

    # We multiply each channel in the feature map array
    # by "how important this channel is" with regard to the top predicted class
    # then sum all the channels to obtain the heatmap class activation
    last_conv_layer_output = last_conv_layer_output[0]
    heatmap = last_conv_layer_output @ pooled_grads[..., tf.newaxis]
    heatmap = tf.squeeze(heatmap)

    # For visualization purpose, we will also normalize the heatmap between 0 & 1
    heatmap = tf.maximum(heatmap, 0) / tf.math.reduce_max(heatmap)
    return heatmap.numpy()

img = load_img('/tmp/Dataset/train/transverse/case10_1/case_M_20181113100307_0U62372111373406_1_005_001-1_a14_ayy_image0006.jpg')

img

# Prepare image
img_array = get_img_array(img_path, size=img_size)
# Remove last layer's softmax
model.layers[-1].activation = None

# Print what the top predicted class is
preds = model.predict(img_array)
#print("Predicted:", decode_predictions(preds, top=1)[0])

# Generate class activation heatmap
heatmap = make_gradcam_heatmap(img_array, model, last_conv_layer_name)

# Display heatmap
plt.style.use('dark_background')
plt.matshow(heatmap)
plt.show()



#Saving and displaying the Grad-CAM visualization on the original image


def save_and_display_gradcam(img_path, heatmap, cam_path, alpha=0.9):
    # Load the original image
    img = keras.preprocessing.image.load_img(img_path)
    img = keras.preprocessing.image.img_to_array(img)

    # Rescale heatmap to a range 0-255
    heatmap = np.uint8(255 * heatmap)

    # Use jet colormap to colorize heatmap
    jet = cm.get_cmap("jet")

    # Use RGB values of the colormap
    jet_colors = jet(np.arange(256))[:, :3]
    jet_heatmap = jet_colors[heatmap]

    # Create an image with RGB colorized heatmap
    jet_heatmap = keras.preprocessing.image.array_to_img(jet_heatmap)
    jet_heatmap = jet_heatmap.resize((img.shape[1], img.shape[0]))
    jet_heatmap = keras.preprocessing.image.img_to_array(jet_heatmap)

    # Superimpose the heatmap on original image
    superimposed_img = jet_heatmap * alpha + img
    superimposed_img = keras.preprocessing.image.array_to_img(superimposed_img)

    # Save the superimposed image
    superimposed_img.save(cam_path)
    # Display Grad CAM
    display(Image(cam_path))


save_and_display_gradcam(img_path, heatmap, "gradcam_101.png")



#Processing and analyzing the model predictions and evaluation results



predictions = new_model.predict(test_gen)
predicted_classes = np.argmax(predictions, axis=1)

print(predicted_classes)

true_classes = []
for _, labels in test_gen:
    true_classes.extend(np.argmax(labels, axis=1))

print(true_classes)

with open("./result_movinet.txt",'w') as f:
    for k in results.history.keys():
        print(k,file=f)
        for i in results.history[k]:
            print(i,file=f)

np.mean(results.history['acc'])

np.mean(results.history['auc_2'])

np.mean(results.history['val_acc'])

np.mean(results.history['val_auc_2'])

np.mean(results.history['f1_score'])

np.mean(results.history['val_f1_score'])

np.mean(results.history['loss'])

np.mean(results.history['val_loss'])

r=results



# Visualization of Model Accuracy and Loss over Epochs



import matplotlib.pyplot as plt

# summarize history for accuracy
plt.plot(r.history['acc'])
plt.plot(r.history['val_acc'])
plt.title('Model Accuracy')
plt.ylabel('Accuracy')
plt.xlabel('Epoch')
plt.legend(['Train', 'Validation'], loc='best')
plt.savefig('incp_sgd_cnn_acc_6..svg',format='svg', dpi=1200)
plt.show()
# summarize history for loss
plt.plot(r.history['loss'])
plt.plot(r.history['val_loss'])
plt.title('Model Loss')
plt.ylabel('Loss')
plt.xlabel('Epoch')
plt.legend(['Train', 'Validation'], loc='upper left')
plt.savefig('incp_sgd_cnn_loss_6..svg',format='svg', dpi=1200)
plt.show()

import matplotlib.pyplot as plt


# summarize history for accuracy



plt.plot(r.history['auc_1'])
plt.plot(r.history['val_auc_1'])
plt.title('Model AUC')
plt.ylabel('AUC')
plt.xlabel('Epoch')
plt.legend(['Train', 'Validation'], loc='best')
plt.savefig('incp_sgd_cnn_auc_6.svg',format='svg', dpi=1200)
plt.show()

import matplotlib.pyplot as plt

# summarize history for accuracy
plt.plot(r.history['f1_score'])
plt.plot(r.history['val_f1_score'])
plt.title('Model F1 Score')
plt.ylabel('F1 Score')
plt.xlabel('Epoch')
plt.legend(['Train', 'Validation'], loc='best')
plt.savefig('incp_sgd_cnn_f1_Score_6.svg',format='svg', dpi=1200)
plt.show()

#Plot the confusion matrix. Set Normalize = True/False



def plot_confusion_matrix(cm, classes, normalize=True, title='Confusion matrix', cmap=plt.cm.Greens):
    """
    This function prints and plots the confusion matrix.
    Normalization can be applied by setting `normalize=True`.
    """
    plt.figure(figsize=(6,6))
    plt.imshow(cm, interpolation='nearest', cmap=cmap)
    plt.title(title)
    plt.colorbar().outline.set_visible(False)
    plt.box(False)
    tick_marks = np.arange(len(classes))
    plt.xticks(tick_marks, classes,fontsize=10)
    plt.yticks(np.arange(0,4),classes)

    if normalize:
            cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]
            cm = np.around(cm, decimals=2)
            cm[np.isnan(cm)] = 0.0
            print("Normalized confusion matrix")
    else:
            print('Confusion matrix, without normalization')
    thresh = cm.max() / 2
    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):
        plt.text(j, i, cm[i, j],
                 horizontalalignment="center",
                 color="white" if cm[i, j] > thresh else "black",fontsize=12)
    plt.ylabel('True label',ha='center', labelpad=-5)
    plt.xlabel('Predicted label')
    plt.savefig('test_cm.svg',format='svg', dpi=1200)



#Print the Target names


from sklearn.metrics import classification_report, confusion_matrix
import itertools
target_names = []
for key in train_generator.class_indices:
    target_names.append(key)

#Confution Matrix



Y_pred = model.predict_generator(test_gen)
y_pred = np.argmax(Y_pred, axis=1)
print('Confusion Matrix')


cm = confusion_matrix(test_gen.classes, y_pred)
plot_confusion_matrix(cm, target_names, title='Confusion Matrix')


#Print Classification Report


print('Classification Report')
print(classification_report(test_gen.classes, y_pred, target_names=target_names))


# Confusion Matrix and Classification Report for Test Set


from pandas import DataFrame
import numpy as np
import seaborn as sn
from sklearn.metrics import classification_report, confusion_matrix
import itertools
target_names = []
for key in train_generator.class_indices:
    target_names.append(key)
Y_pred = model.predict_generator(test_gen)
y_pred = np.argmax(Y_pred, axis=1)
classes=['0','1','2','3']
columns = ['%s' %(i) for i in (classes)]

confm = confusion_matrix(test_gen.classes, y_pred)
df_cm = DataFrame(confm, index=columns, columns=columns)
annot_kws={'fontsize':20,
           'fontstyle':'oblique',
           'alpha':0.9,
           'verticalalignment':'center'}

ax = sn.heatmap(df_cm, cmap='Reds', annot=True,annot_kws=annot_kws,fmt="d")
plt.ylabel('True label',ha='center')
plt.xlabel('Predicted label')
plt.savefig('test_cm.svg',format='svg', dpi=1200)
print('Classification Report')
print(classification_report(test_gen.classes, y_pred, target_names=target_names))



from sklearn.metrics import confusion_matrix

cm = confusion_matrix(true_classes, predicted_classes)
print(cm)

import matplotlib.pyplot as plt
import seaborn as sns

plt.figure(figsize = (10,7))
sns.heatmap(cm, annot=True, fmt='d')
plt.xlabel('Predicted')
plt.ylabel('Truth')
plt.show()

!pip install -U numpy

!pip install scikit-learn

